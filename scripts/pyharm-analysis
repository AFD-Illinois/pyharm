#!/usr/bin/env python3

__license__ = """
 File: pyharm-analysis
 
 BSD 3-Clause License
 
 Copyright (c) 2020-2023, Ben Prather and AFD Group at UIUC
 All rights reserved.
 
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are met:
 
 1. Redistributions of source code must retain the above copyright notice, this
    list of conditions and the following disclaimer.
 
 2. Redistributions in binary form must reproduce the above copyright notice,
    this list of conditions and the following disclaimer in the documentation
    and/or other materials provided with the distribution.
 
 3. Neither the name of the copyright holder nor the names of its
    contributors may be used to endorse or promote products derived from
    this software without specific prior written permission.
 
 THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
 FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
"""

import os, sys
import click
import glob
import psutil
import multiprocessing
import warnings
import h5py
import inspect
import textwrap

import pyharm
from pyharm import io
from pyharm.parallel import calc_nthreads
from pyharm.ana.analysis import *
import pyharm.io.iharm3d_header as hdr

# Use mpi4py if available, or multiprocessing
try:
    import mpi4py
    from mpi4py import MPI
    from mpi4py.futures import MPICommExecutor, MPIPoolExecutor
    if MPI.COMM_WORLD.Get_rank() == 0:
        print("MPI Available", file=sys.stderr)
    # Use MPI unless we're alone
    use_mpi = (MPI.COMM_WORLD.Get_size() > 1)
    def do_out():
        return MPI.COMM_WORLD.Get_rank() == 0
except ImportError:
    use_mpi = False
    def do_out():
        return True

# This extends the help message to print valid analyses
class ExtensibleCmd(click.Command):
    def format_help(self, ctx, formatter):
        click.Command.format_help(self, ctx, formatter)

        figure_list = inspect.getmembers(pyharm.ana.analyses, \
                predicate=lambda f: inspect.isfunction(f) and f.__module__ == pyharm.ana.analyses.__name__)

        formatter.write("\nValid analyses:\n")
        for fnname, fn in figure_list:
            if fnname[0] != "_":
                formatter.write("  "+fnname+":\n")
                if pyharm.ana.analyses.__dict__[fnname].__doc__ is not None:
                    # Take out all whitespace from the docstring
                    docstring = textwrap.shorten(pyharm.ana.analyses.__dict__[fnname].__doc__, 1e6).replace("\n"," ")
                    # Reflow and indent
                    formatter.write(textwrap.indent(textwrap.fill(docstring), "    ") + "\n\n")

@click.command(cls=ExtensibleCmd)
@click.argument('ana_types', nargs=1)
@click.argument('paths', nargs=-1, type=click.Path(exists=True))
@click.option('--paths_are_files', is_flag=True, help="Paths are single files, treat as a single set")
# Common options
@click.option('-s', '--start', default=None, help="Only consider dumps after this time, in M")
@click.option('-e', '--end', default=None, help="Only consider dumps after this time, in M")
@click.option('-as', '--average_start', default=None, help="Beginning time for time-averages")
@click.option('-ae', '--average_end', default=None, help="End time for time-averages")
@click.option('-a', '--append', is_flag=True, help="Append to existing results file, rather than starting fresh")
@click.option('-f', '--filename', default="eht_out.h5", help="Name of each result file")
@click.option('-o', '--output_dir', default=".", help="Base directory for outputs. Any folder structure of models will be preserved.")
@click.option('--prefer_iharm3d', is_flag=True, help="Prefer iharm3d-format files if available alongside KHARMA output")
# Extras
@click.option('-n', '--nthreads', default=None, help="Number of processes to use, if not using MPI")
@click.option('-d', '--debug', is_flag=True, default=False, help="Serial operation for debugging")
@click.option('--nompi', is_flag=True, default=False, help="Avoid MPI even if available")
#@click.option('-m', '--memory_limit', default=1, help="Memory limit in GB for each process, enforced by starting total/m processes.")
def analysis(ana_types, paths, **kwargs):
    """Analyze the simulation output at each of PATHS with the function/combo specified by ANA_TYPES

    This script is designed for large time-average and time-domain operations, where scripts treating individual
    dump files or analyses one at a time are too slow.  Analysis output is batched together in the form of an HDF5
    summary file in a simple format.  This can be read with ana_results.py or your own plotting scripts.

    Each PATH can contain dump files in any format readable by pyharm, either in the given directory or a subdirectory
    named "dumps", "dumps_kharma" or similar.

    ANA_TYPE is a comma-separated list containing one or more of the valid analysis types listed after the arguments.  The
    'basic' analysis is always run, as it pulls data used in subsequent analyses (and is very quick).

    If run within an MPI job/allocation with mpi4py installed, pyharm-analysis will attempt to use all allocated nodes to generate
    frames.  YMMV wildly with MPI installations, with mpi4py installed via pip generally a better choice than through conda.

    """
    path_dirs = [p for p in paths if os.path.isdir(p)]

    # Only treat files if *none* of the args is a directory
    if len(path_dirs) == 0:
        path_dirs = paths

    # If we're treating files together, just treat "." as the only "directory"
    if not kwargs['paths_are_files']:
        if do_out(): print("Generating {} analyses sequentially".format(len(path_dirs)), file=sys.stderr)
    else:
        if do_out(): print("Analyzing {} files as a set".format(len(paths)), file=sys.stderr)
        path_dirs = "."

    # We stay in the *current* folder structure with our targets.
    # But we'll open *outputs* under the desired output_dir if different
    start_path = os.path.abspath(os.getcwd())
    if 'PYHARM_DUMPS_DIR' in os.environ:
        base_path = os.path.abspath(os.environ['PYHARM_DUMPS_DIR'])
    else:
        base_path = start_path
    if 'PYHARM_OUTPUT_DIR' in os.environ and kwargs['output_dir'] == ".":
        out_path = os.path.abspath(os.environ['PYHARM_OUTPUT_DIR'])
    else:
        # Defaults to current dir
        out_path = os.path.abspath(kwargs['output_dir'])

    for path in path_dirs:
        if do_out(): print("Running analyses {} on model {}".format(ana_types, path), file=sys.stderr)
        # If a path...
        if os.path.isdir(path):
            # change dir to path we want to image
            os.chdir(path)
            # Try to load known filenames
            if not kwargs['paths_are_files']:
                files = pyharm.io.get_fnames(".", prefer_iharm3d=kwargs['prefer_iharm3d'])
            else:
                files = paths
        else:
            # Just the single dump file. TODO move to its dir?
            files = [path]

        # Add these so we can pass on just the args
        kwargs['ana_types'] = ana_types
        kwargs['path'] = path
        # Canonical start & end time for averages
        tend = pyharm.io.get_dump_time(files[-1])
        if 'average_start' in kwargs and kwargs['average_start'] is not None:
            kwargs['t_avg_start'] = float(kwargs['average_start'])
        else:
            kwargs['t_avg_start'] = tend / 2

        if 'average_start' in kwargs and kwargs['average_end'] is not None:
            kwargs['t_avg_end'] = float(kwargs['average_end'])
        else:
            kwargs['t_avg_end'] = tend

        # Set filename from analysis if doing one
        if (kwargs['filename'] == 'eht_out.h5') and not (',' in kwargs['ana_types']):
            kwargs['filename'] = kwargs['ana_types']+"_out.h5"

        if do_out():
            # Initialize the file, creating parent dir if not present
            parent_path = os.getcwd().replace(base_path, out_path)
            file_path = os.path.join(parent_path, kwargs['filename'])
            if kwargs['append']:
                outfile = h5py.File(file_path, 'a')
            else:
                if not os.path.exists(parent_path):
                    os.makedirs(parent_path)
                outfile = h5py.File(file_path, 'w')
            # Copy in the header, either as-is or converted
            is_ilhdf = False
            with h5py.File(files[0],'r') as inf:
                if 'header' in inf:
                    hdr_preserve = hdr.hdf5_to_dict(inf['header'])
                    if not 'header' in outfile:
                        outfile.create_group('header')
                    hdr.dict_to_hdf5(hdr_preserve, outfile['header'])
                    is_ilhdf = True
            if not is_ilhdf:
                # Just put in a written one
                hdr_preserve = pyharm.io.read_hdr(files[0])
                hdr.write_hdr(hdr_preserve, outfile)

            # Copy in any diagnostics that are present
            try:
                fname = glob.glob("*.hst")[0]
                if do_out(): print("Loading diag file {}".format(fname), file=sys.stderr)
                diag = pyharm.io.read_log(fname)
                if not 'diag' in outfile:
                    outfile.create_group('diag')
                hdr.dict_to_hdf5(diag, outfile['diag'])
            except (IndexError, IOError):
                # But don't cry if we can't
                pass
            # And record pretty much the only relevant command-line parameters
            outfile['avg/start'] = kwargs['t_avg_start']
            outfile['avg/end'] = kwargs['t_avg_end']

        if kwargs['debug']:
            # Import profiling only if used, start it
            import cProfile, pstats, io
            from pstats import SortKey
            from pympler import tracker
            pr = cProfile.Profile()
            pr.enable()
            # Run sequentially to make profiling & backtraces work
            for n,fname in enumerate(files):
                tr = tracker.SummaryTracker()
                out = analyze((fname, kwargs))
                write_ana_dict(out, outfile, n, len(files))
                tr.print_diff()
            outfile.close()

            pr.disable()
            s = io.StringIO()
            ps = pstats.Stats(pr, stream=s).sort_stats(SortKey.TIME)
            ps.print_stats(10)
            print(s.getvalue())

        else:
            # Suppress math warnings, some are expected
            if not sys.warnoptions:
                warnings.simplefilter("ignore")
            # Try to guess how many processes before we MemoryError out
            if 'nthreads' not in kwargs or kwargs['nthreads'] is None:
                if 'memory_limit' in kwargs and kwargs['memory_limit'] is not None:
                    header = pyharm.io.read_hdr(files[0])
                    nthreads = min(calc_nthreads(header, pad=0.6),
                                psutil.cpu_count())
                else:
                    nthreads = psutil.cpu_count()
            else:
                nthreads = int(kwargs['nthreads'])

            # This application is entirely side-effects (frame creation)
            # So we map & ignore result
            # TODO pass single figure & blit/encode within Python?
            if use_mpi and not kwargs['nompi']:
                print("Using MPI")
                # Everything is set by MPI.  We inherit a situation and use it
                with MPICommExecutor() as executor:
                    # Only the master process gets an executor object here,
                    # so the following is done only once
                    if executor is not None:
                        print("Analyzing {} files across pool size {}".format(len(files), MPI.COMM_WORLD.Get_size()), file=sys.stderr)
                        args = zip(files, (kwargs,)*len(files))
                        # This probably shouldn't be a straight map(), or we might run out of memory on the master node
                        for n, result in enumerate(executor.map(analyze_catch_err, args)):
                            write_ana_dict(result, outfile, n, len(files))
                        outfile.close()
            else:
                print("Using {} processes".format(nthreads))
                with multiprocessing.Pool(nthreads) as pool:
                    args = zip(files, (kwargs,)*len(files))
                    out_iter = pool.imap(analyze_catch_err, args)
                    for n, result in enumerate(out_iter):
                        write_ana_dict(result, outfile, n, len(files))
                outfile.close()



        if do_out(): print("Ran analyses {} on model {}".format(ana_types, path), file=sys.stderr)
        # Change back to wherever our initial directory was, as argument paths can be relative
        os.chdir(start_path)




if __name__ == "__main__":
  analysis()
