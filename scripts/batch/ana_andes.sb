#!/bin/bash -i

#SBATCH -J pyharm_ana
#SBATCH -A ast171
#SBATCH -N 1
# Only 'batch' or 'gpu'
#SBATCH -p batch
#SBATCH -t 12:00:00
#SBATCH -o out-%j.txt

# Run analysis as an MPI job on Andes.
# See pyharm-analysis docs/help for options, test on a few dumps
# before committing to big jobs.

# '-i' argument is for interactive shell to allow running 'conda' 
# Remove if you have a jacked .bashrc and get worse errors than job warnings

# Choose an appropriate number of processes. TODO more intelligence
# We use spread-job below to make sure this actually results in fewer procs/node
if [[ "$*" == *"448"* ]]
then
  NPROC=16
else
  NPROC=32
fi

# Reloading the 'python' module messes with anaconda environment,
# just have it loaded before submitting/as a default module.
#module load python
# Sourcing the conda script doesn't seem to fully work anymore,
# so we run as interactive with '-i' above, which is not ideal
#source /sw/andes/python/3.7/anaconda-base/etc/profile.d/conda.sh
conda activate pyharm

# Better to oversubscribe CPU than serialize as there are
# relatively few numpy ops
export MKL_NUM_THREADS=8
export OMP_NUM_THREADS=8

# Start 1 task/cpu by default. -O option allows more tasks,
# if this proves efficient for simple stuff.
if [[ "$*" != *"-d "* ]]
then
  srun -n $(($SLURM_JOB_NUM_NODES * $NPROC)) --cpu-bind=no --spread-job pyharm-analysis "${@}"
else
  pyharm-analysis "${@}"
fi
